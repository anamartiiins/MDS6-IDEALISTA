{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal imports\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#Specific imports\n",
    "import geopandas\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Internal imports\n",
    "from src.data_extraction.data_extraction import extract_initial_data\n",
    "from src.constants import (\n",
    "    NEW_COLUMNS_NAMES,\n",
    "    REMOVE_COLUMNS_BY_INPUT,\n",
    "    REMOVE_COLUMNS_BY_CORRELATIONS,\n",
    ")\n",
    "from src.preprocessing.preprocessing_utils import (\n",
    "    remove_duplicated_anuncios_id,\n",
    "    find_single_value_columns,\n",
    "    treatment_missing_values,\n",
    "    correlation_values,\n",
    "    feature_engineering,\n",
    "    detect_outliers,\n",
    "    hist_plot_outliers\n",
    ")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all dataset available, provided by idealista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_ine, df_osm, df_pois, df_polygons = extract_initial_data(\n",
    "    root_dir=\"input_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change columns names to friendly ones\n",
    "df = df.drop(columns=[\"ADTYPOLOGY\", \"ADOPERATION\"])\n",
    "df.columns = NEW_COLUMNS_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Division train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['precio']) \n",
    "y = df['precio']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygons and INE censal polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_polygons_and_ine(df_polygons, df_ine, df):\n",
    "    # Convert WKT strings to Shapely geometries and create a GeoDataFrame\n",
    "    df_polygons['geometry'] = df_polygons['WKT'].apply(wkt.loads)\n",
    "    gdf_polygons = geopandas.GeoDataFrame(df_polygons['geometry'], crs='epsg:4326')\n",
    "\n",
    "    # Add additional columns to the GeoDataFrame\n",
    "    gdf_polygons['barrio_id'] = df_polygons['LOCATIONID']\n",
    "    gdf_polygons['barrio'] = df_polygons['LOCATIONNAME']\n",
    "\n",
    "    # Create Point geometries using longitude and latitude coordinates from df_train\n",
    "    geometry = [Point(xy) for xy in zip(df.longitud, df.latitud)]\n",
    "\n",
    "    # Create a GeoDataFrame gdf_ads with df_prices data and geometry column\n",
    "    gdf_train_train = GeoDataFrame(df, crs=\"EPSG:4326\", geometry=geometry)\n",
    "\n",
    "    # Apply a logarithmic scale transformation to the 'precio' column in gdf_ads\n",
    "    gdf_train_train['precio_logaritmico'] = np.log(gdf_train_train['precio'])\n",
    "\n",
    "    # Convert WKT strings to Shapely geometries and create a GeoDataFrame for census polygons\n",
    "    df_ine['geometry'] = df_ine['WKT'].apply(wkt.loads)\n",
    "    gdf_polygons_census = geopandas.GeoDataFrame(df_ine['geometry'], crs='epsg:4326')\n",
    "\n",
    "    # Add additional column 'CUSEC' to the GeoDataFrame representing census polygons\n",
    "    gdf_polygons_census['cusec'] = df_ine['CUSEC']\n",
    "\n",
    "    # Add the census codes (CUSEC)\n",
    "    gdf_train_train = geopandas.sjoin(gdf_train_train, gdf_polygons_census, how=\"inner\")\n",
    "\n",
    "    # Drop index_right \n",
    "    gdf_train_train = gdf_train_train.drop(columns=['index_right'])\n",
    "\n",
    "    # Now add the idealista zones (LOCATIONID, LOCATIONNAME)\n",
    "    gdf_train_train = geopandas.sjoin(gdf_train_train, gdf_polygons, how=\"inner\")\n",
    "\n",
    "    # Drop index_right \n",
    "    gdf_train_train = gdf_train_train.drop(columns=['index_right'])\n",
    "\n",
    "    return gdf_train_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = get_info_from_polygons_and_ine(df_polygons=df_polygons, df_ine=df_ine, df=df_train)\n",
    "df_test = get_info_from_polygons_and_ine(df_polygons=df_polygons, df_ine=df_ine, df=df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INE Censal Polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSETS - Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See general statistics of df\n",
    "description_df = df_train.describe(percentiles=[0.995, 0.97]).transpose().style.format(\"{:.2f}\")\n",
    "description_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated anuncios_id\n",
    "df_train = remove_duplicated_anuncios_id(df_assets=df_train, criteria=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns by input\n",
    "df_train = df_train.drop(columns=REMOVE_COLUMNS_BY_INPUT)\n",
    "df_test = df_test.drop(columns=REMOVE_COLUMNS_BY_INPUT)\n",
    "print('Removed columns:', REMOVE_COLUMNS_BY_INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that only have one different value\n",
    "remove_unique_value_columns = find_single_value_columns(df=df_train)\n",
    "df_train = df_train.drop(columns=remove_unique_value_columns)\n",
    "df_test = df_test.drop(columns=remove_unique_value_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "df_train = treatment_missing_values(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ano Construccion\n",
    "antiguidade_ano_construccion=2018-df_train['ano_construccion']\n",
    "print('Min antiguidade ano construccion', antiguidade_ano_construccion.min())\n",
    "print('Max antiguidade ano construccion', antiguidade_ano_construccion.max())\n",
    "antiguidade_cat_ano_construccion=2018-df_train['cat_ano_construccion']\n",
    "print('Min antiguidade cat ano construccion', antiguidade_cat_ano_construccion.min())\n",
    "print('Max antiguidade cat ano construccion', antiguidade_cat_ano_construccion.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Treatment and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean price by barrio\n",
    "df_metrics_barrios= df_train.groupby(['barrio']).agg({'precio':['median', 'mean', 'std'], \n",
    "                                   'precio_unitario_m2':['median', 'mean', 'std']}).reset_index()\n",
    "\n",
    "df_metrics_barrios.columns = ['barrio', \n",
    "                         'precio_median_barrio', 'precio_mean_barrio', 'precio_std_barrio', \n",
    "                         'precio_unitario_m2_median_barrio', 'precio_unitario_m2_mean_barrio', 'precio_unitario_m2_std_barrio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mean price by barrio to dataset\n",
    "df_train=df_train.merge(df_metrics_barrios[['barrio', 'precio_mean_barrio', 'precio_unitario_m2_mean_barrio']], on=['barrio'], how=\"inner\")\n",
    "df_test=df_test.merge(df_metrics_barrios[['barrio', 'precio_mean_barrio', 'precio_unitario_m2_mean_barrio']], on=['barrio'], how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: add new variables\n",
    "(\n",
    "    add_columns,\n",
    "    remove_columns_by_creating_new_variables,\n",
    "    df_train,\n",
    ") = feature_engineering(df=df_train)\n",
    "\n",
    "print(\"Columns added:\", add_columns)\n",
    "print('Columns removed by adding new:', remove_columns_by_creating_new_variables)\n",
    "\n",
    "(\n",
    "    add_columns,\n",
    "    remove_columns_by_creating_new_variables,\n",
    "    df_test,\n",
    ") = feature_engineering(df=df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation values\n",
    "correlation_matrix, correlated_variables = correlation_values(df=df_train, threshold=0.8)\n",
    "# Remove columns by high correlations\n",
    "df_train = df_train.drop(columns=REMOVE_COLUMNS_BY_CORRELATIONS)\n",
    "\n",
    "df_test = df_test.drop(columns=REMOVE_COLUMNS_BY_CORRELATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_row_before=df_train.shape[0]\n",
    "\n",
    "variables_most_correlated_w_target=['n_banos','n_habitaciones', 'area_construida', 'distancia_castellana']\n",
    "\n",
    "percentile_995_values = {}\n",
    "\n",
    "for var in variables_most_correlated_w_target:\n",
    "    percentile_995_values[var] = df_train[var].quantile(0.995)\n",
    "\n",
    "print(percentile_995_values)\n",
    "\n",
    "for var in variables_most_correlated_w_target:\n",
    "    hist_plot_outliers(df=df_train[df_train[var] > percentile_995_values[var]],name_variable = var)\n",
    "\n",
    "condition_to_exclude_outliers = ((df_train['n_banos']>percentile_995_values['n_banos']) \n",
    "                                 | (df_train['n_habitaciones']>percentile_995_values['n_habitaciones']) \n",
    "                                 | (df_train['area_construida']>percentile_995_values['area_construida']) \n",
    "                                 | (df_train['distancia_castellana']>percentile_995_values['distancia_castellana']))\n",
    "\n",
    "df_train=df_train[~condition_to_exclude_outliers]\n",
    "\n",
    "nr_row_after=df_train.shape[0]\n",
    "\n",
    "print('Percentage of rows deleted: ', 1-nr_row_after/nr_row_before)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME\n",
    "# Analyse target outliers: inter quartis\n",
    "outliers = detect_outliers(df=df_train, column_name='precio', threshold=10)\n",
    "outliers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N_Banos: Impute values when n_banos = 0 --> train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where n_banos > 0\n",
    "filtered_df = df_train[df_train['n_banos'] > 0]\n",
    "\n",
    "# Calculate number of bathrooms per square meter\n",
    "filtered_df['n_banos_m2'] = filtered_df['n_banos'] / filtered_df['area_construida']\n",
    "\n",
    "# Calculate mean number of bathrooms per square meter\n",
    "mean_bathrooms_per_sqm = filtered_df['n_banos_m2'].median()\n",
    "\n",
    "# Impute number of bathrooms for rows where n_banos == 0: mean number of bath by m^2 * m^2, rounded, and minimum 1\n",
    "df_train['n_banos_m2'] = (np.maximum(mean_bathrooms_per_sqm * df_train['area_construida'],1)).round().astype(int)\n",
    "\n",
    "# Validate if it is a good way to values, calculating mape comparing with the real n_banos\n",
    "df_train_aux = df_train[df_train['n_banos']>0]\n",
    "absolute_percentage_errors = np.abs((df_train_aux['n_banos'] - df_train_aux['n_banos_m2']) / df_train_aux['n_banos'])\n",
    "mape = np.mean(absolute_percentage_errors) * 100\n",
    "print(\"MAPE\", mape)\n",
    "\n",
    "# Delete filtered_df, df_train_aux as they are only auxiliar \n",
    "del filtered_df, df_train_aux\n",
    "\n",
    "# # Assign the imputed value to n_banos where n_banos == 0. All houses with 0 bathrooms are houses to renovate\n",
    "df_train.loc[df_train['n_banos'] == 0, 'n_banos'] = df_train.loc[df_train['n_banos'] == 0, 'n_banos_m2']\n",
    "\n",
    "# # # Drop the n_banos_m2 column as it is no longer needed\n",
    "df_train = df_train.drop(columns=['n_banos_m2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportar df_train_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(r'output_data\\df_train_util.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.fillna(0).to_csv(r'output_data\\df_test_util.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare data types for common columns\n",
    "for col in df_train.columns.intersection(df_test.columns):\n",
    "    dtype_df1 = df_train[col].dtype\n",
    "    dtype_df2 = df_test[col].dtype\n",
    "    if dtype_df1 != dtype_df2:\n",
    "        print(f\"Data type of column '{col}' differs between DataFrames:\")\n",
    "        print(f\"- DataFrame 1: {dtype_df1}\")\n",
    "        print(f\"- DataFrame 2: {dtype_df2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test.latitud > 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
