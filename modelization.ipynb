{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from src.evaluation.evaluation import calculate_metrics, export_model, save_graph_feature_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import tree, ensemble\n",
    "import xgboost\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict, GroupKFold\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from src.modelization.models_utils import get_pipeline\n",
    "from src.constants import BASE_PATH_EXPERIMENTS, PATH_EVALUATION_DF_WITH_METRICS_CSV, PATH_EVALUATION_CSV, PATH_TRAIN, PATH_TEST\n",
    "from datetime import datetime\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import warnings\n",
    "import zipfile\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Data\n",
    "df_train = pd.read_csv(PATH_TRAIN)\n",
    "df_test = pd.read_csv(PATH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_drop=['geometry', 'barrio_id', 'barrio']\n",
    "# df_train = df_train.drop(columns=columns_to_drop)\n",
    "# df_test = df_test.drop(columns=columns_to_drop)\n",
    "df_test[\"precio_logaritmico_mean_barrio\"]=np.log(df_test['precio_mean_barrio'])\n",
    "\n",
    "# Evaluate with different targets: precio, precio_unitario_m2, precio_logaritmico\n",
    "possible_targets= [\"precio\", \"precio_unitario_m2\", \"precio_logaritmico\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precio\n",
      "precio_unitario_m2\n",
      "precio_logaritmico\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty dataframes\n",
    "evaluation_df_with_metrics = pd.DataFrame()\n",
    "evaluation = pd.DataFrame()\n",
    "\n",
    "for target in possible_targets:\n",
    "    baseline_model = f'Precio medio por barrio - {target}'\n",
    "\n",
    "    # Calculate metrics based on the current target\n",
    "    if target == 'precio':\n",
    "        metrics_df, df_test_with_metrics = calculate_metrics(df_test['precio'], df_test['precio_mean_barrio'], df_test, model_name=baseline_model, target=target)\n",
    "    elif target == 'precio_unitario_m2':\n",
    "        metrics_df, df_test_with_metrics = calculate_metrics(df_test['precio_unitario_m2'], df_test['precio_unitario_m2_mean_barrio'], df_test, model_name=baseline_model, target=target)\n",
    "    else:\n",
    "        metrics_df, df_test_with_metrics = calculate_metrics(df_test['precio_logaritmico'], df_test['precio_logaritmico_mean_barrio'], df_test, model_name=baseline_model, target=target)\n",
    "\n",
    "    # Append results to the evaluation dataframes\n",
    "    df_test_with_metrics['model_name'] = baseline_model\n",
    "    evaluation_df_with_metrics = pd.concat([evaluation_df_with_metrics, df_test_with_metrics], ignore_index=True)\n",
    "    evaluation = pd.concat([evaluation, metrics_df], ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=['precio', 'precio_unitario_m2', \"precio_logaritmico\"])\n",
    "y_train = df_train[target]\n",
    "X_test = df_test.drop(columns=['precio', 'precio_unitario_m2', \"precio_logaritmico\"])\n",
    "y_test = df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = tree.DecisionTreeRegressor()\n",
    "model_rf = ensemble.RandomForestRegressor()\n",
    "model_gb = ensemble.GradientBoostingRegressor()\n",
    "model_xgb = xgboost.XGBRegressor()\n",
    "\n",
    "models = {\n",
    "    'Decision Tree': {\n",
    "        'model': model_dt,\n",
    "        'param_grid': {\n",
    "            'model__max_depth': [5, 10, 20],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': model_rf,\n",
    "        'param_grid': {\n",
    "            'model__n_estimators': [50, 100, 200],\n",
    "            'model__max_depth': [5, 10, 20],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boost': {\n",
    "        'model': model_gb,\n",
    "        'param_grid': {\n",
    "            'model__n_estimators': [50, 100, 200],\n",
    "            'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'model__max_depth': [3, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'eXtreme Gradient Boost': {\n",
    "        'model': model_xgb,\n",
    "        'param_grid': {\n",
    "            'model__n_estimators': [50, 100, 200],\n",
    "            'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'model__max_depth': [3, 5, 10]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipeline = get_pipeline(\n",
    "    base_model=model,\n",
    "    impute=True,  \n",
    "    scale=True,  \n",
    "    encode=True,\n",
    "    num_features=X_train.columns.to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_tunning={\n",
    "                \"Grid_SearchCV\": GridSearchCV(\n",
    "                    estimator=pipeline,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    cv=5\n",
    "                    ),\n",
    "                \"Randomized_SearchCV\" : \n",
    "                     RandomizedSearchCV(estimator=pipeline, \n",
    "                                          param_distributions=param_random,\n",
    "                                          random_state=42, \n",
    "                                          verbose= 1)\n",
    "               #  \"Bayesian_SearchCV\": BayesSearchCV(estimator=pipeline,\n",
    "               #     search_spaces=param_space,\n",
    "               #     random_state=42\n",
    "               #     ),\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Randomized_SearchCV...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "# Iterate through hyper_tunning dictionary\n",
    "for method_name, search_cv in hyper_tunning.items():\n",
    "    print(f\"Performing {method_name}...\")\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    search_cv.fit(X_train, y_train)\n",
    "\n",
    "    # Best model after hyperparameter tuning\n",
    "    best_model = search_cv.best_estimator_\n",
    "\n",
    "    # Perform cross-validation predictions with the best model\n",
    "    y_pred = cross_val_predict(best_model, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "metrics_df, df_test_with_metrics = calculate_metrics(y_train, y_pred, X_train, model_name, target)\n",
    "\n",
    "# Add model_name column to df_test_with_metrics\n",
    "df_test_with_metrics['model_name'] = model_name\n",
    "df_test_with_metrics['target'] = target\n",
    "df_test_with_metrics['model_folder'] = f\"experiment_{model_name}_{datetime.now().strftime('%Y%m%d')}\"\n",
    "\n",
    "# Append df_test_with_metrics to all_df_test_with_metrics\n",
    "evaluation_df_with_metrics = pd.concat([evaluation_df_with_metrics, df_test_with_metrics], ignore_index=True)\n",
    "\n",
    "# Append metrics_df to all_metrics_df\n",
    "evaluation = pd.concat([evaluation, metrics_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barrio = df_train['barrio'].values\n",
    "# group_kfold = GroupKFold(n_splits=5) \n",
    "# city_kfold = group_kfold.split(X_train, y_train, barrio)  \n",
    "# train_indices, test_indices = [list(traintest) for traintest in zip(*city_kfold)]\n",
    "# city_cv = [*zip(train_indices, test_indices)]\n",
    "# predictions = cross_val_predict(model, X_train, y_train, cv=city_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Target</th>\n",
       "      <th>RMSE - Root Mean Squared Error</th>\n",
       "      <th>MAPE - Mean Absolute Percentage Error</th>\n",
       "      <th>R2 - Coefficient of Determination</th>\n",
       "      <th>ME - Mean Error</th>\n",
       "      <th>MED - Median Error</th>\n",
       "      <th>MAE - Mean Absolute Error</th>\n",
       "      <th>MAED - Median Absolute Error</th>\n",
       "      <th>MAPED - Median Absolute Percentage Error</th>\n",
       "      <th>Percentage error lower_5</th>\n",
       "      <th>Percentage error lower_10</th>\n",
       "      <th>Percentage error lower_25</th>\n",
       "      <th>Standard Deviation of Errors</th>\n",
       "      <th>Confidence Interval Lower</th>\n",
       "      <th>Confidence Interval Upper</th>\n",
       "      <th>Model Folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Precio medio por barrio - precio</td>\n",
       "      <td>precio</td>\n",
       "      <td>319,575</td>\n",
       "      <td>50.16</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-1,172</td>\n",
       "      <td>-30,483</td>\n",
       "      <td>171,293</td>\n",
       "      <td>84,913</td>\n",
       "      <td>33.00</td>\n",
       "      <td>8.38</td>\n",
       "      <td>16.80</td>\n",
       "      <td>39.46</td>\n",
       "      <td>319572.42</td>\n",
       "      <td>-496032.06</td>\n",
       "      <td>745708.33</td>\n",
       "      <td>experiment_Precio medio por barrio - precio_20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precio medio por barrio - precio_unitario_m2</td>\n",
       "      <td>precio_unitario_m2</td>\n",
       "      <td>955</td>\n",
       "      <td>21.32</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-12</td>\n",
       "      <td>-85</td>\n",
       "      <td>689</td>\n",
       "      <td>505</td>\n",
       "      <td>15.00</td>\n",
       "      <td>17.63</td>\n",
       "      <td>34.12</td>\n",
       "      <td>70.58</td>\n",
       "      <td>955.40</td>\n",
       "      <td>-1787.63</td>\n",
       "      <td>2175.01</td>\n",
       "      <td>experiment_Precio medio por barrio - precio_un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precio medio por barrio - precio_logaritmico</td>\n",
       "      <td>precio_logaritmico</td>\n",
       "      <td>1</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0</td>\n",
       "      <td>-0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>79.29</td>\n",
       "      <td>97.66</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>0.85</td>\n",
       "      <td>experiment_Precio medio por barrio - precio_lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>precio_logaritmico</td>\n",
       "      <td>0</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>98.86</td>\n",
       "      <td>99.90</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.35</td>\n",
       "      <td>experiment_Random Forest_20240307-203750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Model              Target  \\\n",
       "0              Precio medio por barrio - precio              precio   \n",
       "1  Precio medio por barrio - precio_unitario_m2  precio_unitario_m2   \n",
       "2  Precio medio por barrio - precio_logaritmico  precio_logaritmico   \n",
       "3                                 Random Forest  precio_logaritmico   \n",
       "\n",
       "  RMSE - Root Mean Squared Error  MAPE - Mean Absolute Percentage Error  \\\n",
       "0                        319,575                                  50.16   \n",
       "1                            955                                  21.32   \n",
       "2                              1                                   3.20   \n",
       "3                              0                                   1.02   \n",
       "\n",
       "   R2 - Coefficient of Determination ME - Mean Error MED - Median Error  \\\n",
       "0                               0.40          -1,172            -30,483   \n",
       "1                               0.68             -12                -85   \n",
       "2                               0.54              -0                 -0   \n",
       "3                               0.94               0                  0   \n",
       "\n",
       "  MAE - Mean Absolute Error MAED - Median Absolute Error  \\\n",
       "0                   171,293                       84,913   \n",
       "1                       689                          505   \n",
       "2                         0                            0   \n",
       "3                         0                            0   \n",
       "\n",
       "   MAPED - Median Absolute Percentage Error  Percentage error lower_5  \\\n",
       "0                                     33.00                      8.38   \n",
       "1                                     15.00                     17.63   \n",
       "2                                      3.00                     79.29   \n",
       "3                                      1.00                     98.86   \n",
       "\n",
       "   Percentage error lower_10  Percentage error lower_25  \\\n",
       "0                      16.80                      39.46   \n",
       "1                      34.12                      70.58   \n",
       "2                      97.66                     100.00   \n",
       "3                      99.90                     100.00   \n",
       "\n",
       "   Standard Deviation of Errors  Confidence Interval Lower  \\\n",
       "0                     319572.42                 -496032.06   \n",
       "1                        955.40                   -1787.63   \n",
       "2                          0.50                      -1.15   \n",
       "3                          0.18                      -0.39   \n",
       "\n",
       "   Confidence Interval Upper  \\\n",
       "0                  745708.33   \n",
       "1                    2175.01   \n",
       "2                       0.85   \n",
       "3                       0.35   \n",
       "\n",
       "                                        Model Folder  \n",
       "0  experiment_Precio medio por barrio - precio_20...  \n",
       "1  experiment_Precio medio por barrio - precio_un...  \n",
       "2  experiment_Precio medio por barrio - precio_lo...  \n",
       "3           experiment_Random Forest_20240307-203750  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrames to CSV files\n",
    "evaluation.to_csv(PATH_EVALUATION_CSV, index=False)\n",
    "evaluation_df_with_metrics.to_csv(PATH_EVALUATION_DF_WITH_METRICS_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_to_test = {\n",
    "#                   f'Decision Tree wo CV {target}' : model_dt, \n",
    "#                   f'RandomForest wo CV {target}' : model_rf, \n",
    "#                   f'Gradient Boosting wo CV {target}': model_gb, \n",
    "#                   f'eXtreme Gradient Boost wo CV {target}':model_xgb,\n",
    "#                   f'CatBoost wo CV {target}': model_cb\n",
    "#                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# import pandas as pd\n",
    "\n",
    "# for model_name, model in models_to_test.items():\n",
    "#     # Create pipeline\n",
    "#     pipeline = get_pipeline(\n",
    "#         base_model=model,\n",
    "#         impute=True,  \n",
    "#         scale=True,  \n",
    "#         encode=True,\n",
    "#         num_features=X_train.columns.to_list()\n",
    "#     )\n",
    "    \n",
    "#     # Fit the pipeline and make predictions\n",
    "#     pipeline.fit(X_train, y_train)\n",
    "    \n",
    "#     # Perform cross-validation predictions\n",
    "#     y_pred = cross_val_predict(pipeline, X_train, y_train, cv=5)\n",
    "    \n",
    "#     # Export Model\n",
    "#     output_folder = export_model(\n",
    "#         model=pipeline,\n",
    "#         X_train=X_train,\n",
    "#         y_train=y_train,\n",
    "#         base_path=BASE_PATH_EXPERIMENTS,\n",
    "#         save_model=True,     \n",
    "#         save_datasets=True,  \n",
    "#         zip_files=True      \n",
    "#     )\n",
    "\n",
    "#     # Calculate metrics\n",
    "#     metrics_df, df_test_with_metrics = calculate_metrics(y_train, y_pred, X_train, model_name)\n",
    "\n",
    "#     # Add model_name column to df_test_with_metrics\n",
    "#     df_test_with_metrics['model_name'] = model_name\n",
    "#     df_test_with_metrics['model_folder'] = f\"experiment_{model_name}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "#     # Append df_test_with_metrics to all_df_test_with_metrics\n",
    "#     evaluation_df_with_metrics = pd.concat([evaluation_df_with_metrics, df_test_with_metrics], ignore_index=True)\n",
    "\n",
    "#     # Append metrics_df to all_metrics_df\n",
    "#     evaluation = pd.concat([evaluation, metrics_df], ignore_index=True)\n",
    "    \n",
    "# # Save the DataFrames to CSV files\n",
    "# evaluation.to_csv(PATH_EVALUATION_CSV, index=False)\n",
    "# evaluation_df_with_metrics.to_csv(PATH_EVALUATION_DF_WITH_METRICS_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regressor (model_dt):\n",
    "# max_depth: Typically ranges from 1 to 32.\n",
    "# min_samples_split: Typically ranges from 2 to 20.\n",
    "# min_samples_leaf: Typically ranges from 1 to 10.\n",
    "# max_features: Typically ranges from 1 to the number of features.\n",
    "\n",
    "params_dt = {\"max_depth\": [5, 10, 15], \n",
    "             \"min_samples_split\" : [4, 6, 10 ],\n",
    "             \"max_features\": [15,20]\n",
    "             }\n",
    "\n",
    "# Random Forest Regressor (model_rf):\n",
    "# n_estimators: Typically ranges from 50 to 1000.\n",
    "# max_depth: Typically ranges from 1 to 32.\n",
    "# min_samples_split: Typically ranges from 2 to 20.\n",
    "# min_samples_leaf: Typically ranges from 1 to 10.\n",
    "\n",
    "params_rf = {'n_estimators': [75, 200, 500] ,\n",
    "          'max_depth' : [5,10] ,\n",
    "          'min_samples_split' : [4, 6, 8],\n",
    "          }\n",
    "\n",
    "\n",
    "# Gradient Boosting Regressor (model_gb):\n",
    "# n_estimators: Typically ranges from 50 to 1000.\n",
    "# learning_rate: Typically ranges from 0.01 to 0.1.\n",
    "# max_depth: Typically ranges from 1 to 10.\n",
    "# min_samples_split: Typically ranges from 2 to 20.\n",
    "\n",
    "params_gb = {'n_estimators':[75, 150, 200, 500],\n",
    "             'learning_rate' : [0.05, 0.1, 0.15],\n",
    "             'max_depth' : [5, 10, 15],\n",
    "             'min_samples_split' : [4, 6, 8],\n",
    "             }\n",
    "\n",
    "# XGBoost Regressor (model_xgb):\n",
    "# n_estimators: Typically ranges from 50 to 1000.\n",
    "# learning_rate: Typically ranges from 0.01 to 0.1.\n",
    "# max_depth: Typically ranges from 1 to 10.\n",
    "# min_child_weight: Typically ranges from 1 to 10.\n",
    "\n",
    "params_xgb = {'n_estimators': [75, 150, 200, 500],\n",
    "              'learning_rate' : [0.05, 0.1, 0.15],\n",
    "              'max_depth' : [5, 10, 15],\n",
    "              'min_child_weight' : [2,3,5]}\n",
    "\n",
    "# CatBoost Regressor (model_cb):\n",
    "# n_estimators: Typically ranges from 50 to 1000.\n",
    "# learning_rate: Typically ranges from 0.01 to 0.1.\n",
    "# max_depth: Typically ranges from 1 to 10.\n",
    "# l2_leaf_reg: Typically ranges from 1 to 10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform grid search and save results\n",
    "folder_names_params = {\n",
    "    'experiment_DecisionTreeRegressor_20240303-151554': params_dt,\n",
    "    'experiment_RandomForestRegressor_20240303-151806': params_rf,\n",
    "    'experiment_GradientBoostingRegressor_20240303-151845': params_gb,\n",
    "    'experiment_XGBRegressor_20240303-151849': params_xgb,\n",
    "}\n",
    "\n",
    "for folder, param_grid in folder_names_params.items():\n",
    "    # Path to the zip file\n",
    "    zip_file_path = f'src/evaluation/{folder}/model.zip'\n",
    "    \n",
    "    # Name of the file within the zip folder\n",
    "    file_name_within_zip = 'model.pkl'\n",
    "\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # Extract the file from the zip folder\n",
    "        with zip_ref.open(file_name_within_zip) as file:\n",
    "            # Read the file using pandas\n",
    "            pipeline = pickle.load(file)\n",
    "    \n",
    "    # Grid search for best parameters\n",
    "    gs = GridSearchCV(estimator=pipeline,\n",
    "                      param_grid=param_grid,\n",
    "                      scoring='neg_root_mean_squared_error',\n",
    "                    #   CV=5\n",
    "                      )\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the grid search results for the current model and folder\n",
    "    grid_search_info = {\n",
    "        'best_params': gs.best_params_,\n",
    "        'best_score': gs.best_score_,\n",
    "        'cv_results': gs.cv_results_,\n",
    "    }\n",
    "\n",
    "    # Save the grid search information to a JSON file within the folder\n",
    "    grid_search_output_file = os.path.join(f'src/evaluation/{folder}', 'grid_search_info.json')\n",
    "    with open(grid_search_output_file, 'w') as f:\n",
    "        json.dump(grid_search_info, f)\n",
    "\n",
    "    # Save in the model folder the graph with feature importance\n",
    "    save_graph_feature_importance(model=model, X_train=X_train, folder=folder)\n",
    "\n",
    "print(\"Grid search information saved successfully. Feature Importance graph saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To use CatBoost is needed to transform data into a Pool Object\n",
    "# train_dataset = cb.Pool(X_train, y_train) \n",
    "# test_dataset = cb.Pool(X_test, y_test)\n",
    "# model_cb = cb.CatBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_space = {\n",
    "#     'model__n_estimators': Integer(50, 200),           # Number of trees in the forest\n",
    "#     'model__max_depth': Integer(3, 20),                # Maximum depth of the tree\n",
    "#     'model__min_samples_split': Integer(2, 10),        # Minimum number of samples required to split an internal node\n",
    "#     'model__min_samples_leaf': Integer(1, 4)           # Minimum number of samples required to be at a leaf node\n",
    "# }\n",
    "\n",
    "# param_random = {\n",
    "#     'model__n_estimators': randint(50, 200),           # Number of trees in the forest\n",
    "#     'model__max_depth': randint(3, 20),                # Maximum depth of the tree\n",
    "#     'model__min_samples_split': randint(2, 10),        # Minimum number of samples required to split an internal node\n",
    "#     'model__min_samples_leaf': randint(1, 4)           # Minimum number of samples required to be at a leaf node\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  os.path.exists(PATH_EVALUATION_DF_WITH_METRICS_CSV):\n",
    "    evaluation_df_with_metrics = pd.read_csv(PATH_EVALUATION_DF_WITH_METRICS_CSV)\n",
    "if  os.path.exists(PATH_EVALUATION_CSV):\n",
    "    evaluation = pd.read_csv(PATH_EVALUATION_CSV)\n",
    "\n",
    "# Check if the CSV files exist, and save dataframes accordingly\n",
    "if not os.path.exists(PATH_EVALUATION_DF_WITH_METRICS_CSV):\n",
    "    evaluation_df_with_metrics.to_csv(PATH_EVALUATION_DF_WITH_METRICS_CSV, index=False)\n",
    "else:\n",
    "    existing_df_with_metrics = pd.read_csv(PATH_EVALUATION_DF_WITH_METRICS_CSV)\n",
    "    evaluation_df_with_metrics = pd.concat([existing_df_with_metrics, evaluation_df_with_metrics], ignore_index=True)\n",
    "    evaluation_df_with_metrics.to_csv(PATH_EVALUATION_DF_WITH_METRICS_CSV, index=False)\n",
    "\n",
    "if not os.path.exists(PATH_EVALUATION_CSV):\n",
    "    evaluation.to_csv(PATH_EVALUATION_CSV, index=False)\n",
    "else:\n",
    "    existing_evaluation = pd.read_csv(PATH_EVALUATION_CSV)\n",
    "    evaluation = pd.concat([existing_evaluation, evaluation], ignore_index=True)\n",
    "    evaluation.to_csv(PATH_EVALUATION_CSV, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
